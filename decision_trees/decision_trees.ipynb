{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173b4b29",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "## Introduction \n",
    "\n",
    "Decision trees are a simple yet remarkably powerful type of classification model. Decision trees categorize data by asking a yes/no question based each feature (or a subset of features) from your dataset. Below is an example of a very simple decision tree. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"tree.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f5694",
   "metadata": {},
   "source": [
    "A decision tree repeatedly splits the data at each **node** by proposing a quesition with a binary outcome. The first node is the root node, and the last nodes are the leaf nodes which store our predictions. \n",
    "\n",
    "## Constructing decision trees\n",
    "For our simple made up dataset, it was easy to construct a tree that classified our samples with 100% accuracy. We only have three entries and very few features. However, when handling datasets which are much larger (think thousands of different fruits with many more features) the ideal decision tree structure becomes far less apparent. Constructing a good decision tree involves understanding *what questions to ask, and why to ask them.* We would like a rigorous theoretical framework to arrive at the best possible tree structure. For that, we use two key concepts:\n",
    "\n",
    "1. A measure of *impurity* with the **Gini score**\n",
    "2. A measure of **information gain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91689d7",
   "metadata": {},
   "source": [
    "## Impurity and information gain\n",
    "The Gini score describes the **probability that a label is incorrectly assigned to a randomly chosen example in a set of data.** Passing our data through our example decision tree results in our leaf nodes having zero impurity (0 gini score.) This is because for our three examples, all our leaf nodes end up having exactly one type of fruit (it is impossible to mismatch the one existing label at each node.) The outcome is considered perfectly pure. On the other hand, if we instead introduced another fruit, say a yellow pineapple with height 8cm, then our leftmost leafnode would classify pineapple and banana in the same node. This results in an impurity of 50% (i.e a 50% chance that we classify incorrectly.) \n",
    "\n",
    "Our goal is to choose our questions at each node to *minimize the gini score.* Below is a step by step description of how this is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ba11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
