{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173b4b29",
   "metadata": {},
   "source": [
    "# CART (Classification and Regression Trees)\n",
    "## Introduction \n",
    "Decision trees are a simple yet remarkably powerful models which can be used in both classification and regression problems. Decision trees categorize data by proposing a question based on each feature (or a subset of features) from your dataset, and then splitting the data based on that question. There are many different types of decision trees, but they all work on the same premise: \n",
    "\n",
    "**All decision trees partition the data recursively by examining the features of the data, and choosing the feature(s) that best split the data. In other words, these decisions are based on maximizing data homogenity (similarity) within the trees leaf nodes, and heterogenity (difference) between the leaf nodes.**\n",
    "\n",
    "Since such models are built on the premise of the labeled data we provide them, they belong to the *supervised learning* family of algorithms. This notebook examines the CART algorithm, a common decision tree algorithm which produces binary classification or regression trees based on wether the data is categorical or numeric. \n",
    "\n",
    "# CART for Classification\n",
    "Below is an example of a very simple decision tree for classification. We use this tree as an example to describe how CART works in classification problems. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images_and_data/tree.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f5694",
   "metadata": {},
   "source": [
    "A CART decision tree repeatedly splits the data at each **node** by proposing a question with a binary outcome. The first node is the root node, and the last nodes are the leaf nodes which store our predictions. For our simple made up dataset, it was easy to construct a tree that classified our samples with 100% accuracy. We only have three entries and very few features. However, when handling datasets which are much larger (think thousands of different fruits with many more features) the ideal decision tree structure becomes far less apparent. Constructing a good decision tree involves understanding *what questions to ask, and why to ask them.* Ideally, we want to choose our questions in a way that best classifies our data, and works well at predicting any new data. We would like a rigorous theoretical framework to arrive at the best possible tree structure. CART uses two key concepts to achieve this:\n",
    "\n",
    "1. A measure of *impurity* with the **Gini score**\n",
    "2. A measure of **information gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91689d7",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "The Gini impurity describes the **probability that a label is incorrectly assigned to a randomly chosen example in a set of data.** We pay particular attention to the **difference in Gini score between two sets of data** when creating a partition (we elaborate on this later on.) First, we describe the formula for the Gini impurity.\n",
    "\n",
    "### Gini impurity = $ \\sum_{n=0}^{i} p_{i}~(1~-~p_{i})$\n",
    "where $p_{i}$ is the probability of the label 'i' being chosen. We can simplify this formula:\n",
    "### $ \\sum_{n=0}^{i} p_{i}~(1~-~p_{i}) $ = $ \\sum_{n=0}^{i} p_{i}~-\\sum_{n=0}^{i}~(p_{i})^2 $ \n",
    "### $ \\sum_{n=0}^{i} p_{i} $ = 1\n",
    "Therefore \n",
    "### Gini impurity = $ 1 - \\sum_{n=0}^{i} (p_{i})^2 $\n",
    "\n",
    "<br>\n",
    "\n",
    "Passing our data through our example decision tree results in our leaf nodes having zero impurity (0 gini score.) This is because for our three examples, all our leaf nodes end up having exactly one type of fruit (it is impossible to mismatch the one existing label at each node.) The outcome is considered perfectly pure. On the other hand, if we instead introduced another fruit, say a yellow pineapple with height 8cm, then our leftmost leafnode would classify pineapple and banana in the same node. This results in an impurity of 50% (i.e a 50% chance that we classify incorrectly.) \n",
    "\n",
    "Our goal is to choose our questions at each node to *minimize the gini impurity.* Below is a step by step description of how this is done. \n",
    "\n",
    "1. Compute the impurity of the starting set of data\n",
    "2. Ask a question based on a feature in the data. \n",
    "3. Compute the weighted average gini impurity from the resulting leaf nodes of that question\n",
    "4. Compute the *difference in Gini scores from before the question was asked, and the weighted average Gini impurity in step 3.*\n",
    "5. Repeat steps 2 through 4 for all the features , and choose the question which produces the **biggest drop in Gini impurity**. \n",
    "\n",
    "Note, that if the best reduction in Gini impurity we can obtain is zero, we no longer split the node and it becomes a leaf node. \n",
    "\n",
    "Using these steps, we recursively build each branch of the tree until there are no more questions left to ask. We summarize these steps below using our original made up fruit data table, but slightly modified to have a quantity feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef637d",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"images_and_data/step_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"images_and_data/step_2.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images_and_data/step_3.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"images_and_data/step_4.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images_and_data/step_5.png\" width=300 height=300 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfdeeb",
   "metadata": {},
   "source": [
    "# CART for Regression \n",
    "CART can be used in cases where the target variable is continuous. In such a case, CART assumes a regression framework and needs a measure of how well its predictions model the original data. Just like linear regression, CART takes advantage of the **Least Squares Deviation**, where the goal is to minimize the sum of the squared residuals between the predicted values and the datapoints.\n",
    "\n",
    "## Demonstrating Least Squares using 1 Predictor \n",
    "To demonstrate, we use a made up graph of some fictional populations monthly rent as a function of disposable income. In this case, we are using 1 predictor (income) to try and model montly rent. 1 predictor also makes it easier to demonstrate the least squares regression. \n",
    "\n",
    "<img src=\"images_and_data/data.png\" width=400 height=400 />\n",
    "<br>\n",
    "CART begins its regression by iterating through each datapoint, taking the predictor value and carrying out these steps:  \n",
    "\n",
    "1. Produce a partition based based on the datapoint. \n",
    "2. Compute the residual sum of squares for both children nodes. For each child node, we comute the residual with respect to the average value of the data that falls into each node. \n",
    "3. **Choose the the datapoint which produces the smallest residual sum of squares (RSS) as the partition value at the node.**\n",
    "4. Recursively reapply the same steps to the chidren nodes and build the tree until some stopping criteria is reached\n",
    "\n",
    "Stopping criteria is elaborated on later on.  \n",
    "<table><tr>\n",
    "<td> <img src=\"images_and_data/reg_s1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"images_and_data/reg_partition.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"images_and_data/reg_s2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "<img src=\"images_and_data/reg_s34.png\" width=350 height=400 />\n",
    "<br>\n",
    "Though we only use one predictor in this example, CART can extend this least squares approach to multiple predictors. \n",
    "\n",
    "# Evaluating a CART Model\n",
    "Evaluating a CART DT follows a similar approach to to many other modelling types. We split out data into training  and test sets. The DT is fed a training set to construct the model, and various hyperparameters (such as DT depth, or other custom stopping criteria like minimum number of datapoints per leaf node) can be tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab772f",
   "metadata": {},
   "source": [
    "# CART classification using scikit-learn\n",
    "We write a simple implementation of a CART DT using scikit learn. For our data, we borrow a diabetes dataset which lists a number of patients, some diagnostic data (blood pressure, glucose, BMI) and finally a diabetes diagnosis (yes/no). We aim to construct a CART DT which best predicts the whether a patient has diabetes absed on this diagnostic information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # DT Classifier\n",
    "from sklearn import metrics # Used to compute accuracy of model\n",
    "from sklearn.model_selection import train_test_split # Used to split data into train and test sets\n",
    " \n",
    "# Extract data\n",
    "diabetes_data = pd.read_csv(\"/Users/srujanvajram/Documents/Github/ML_playground/ML_playground/decision_trees/CART/images_and_data/diabetes.csv\")\n",
    "\n",
    "# Grab the predictor variables and target variable separately \n",
    "predictors = diabetes_data.iloc[:, :-1]\n",
    "targets = diabetes_data.iloc[:, -1]\n",
    "\n",
    "# Split data into test and training sets \n",
    "# 80/20 split among training : test\n",
    "predictor_train, predictor_test, target_train, target_test = train_test_split(predictors, \n",
    "                                                                              targets, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=1)\n",
    "\n",
    "\n",
    "# Instantiate classifier\n",
    "CART_classifier = DecisionTreeClassifier(criterion = 'gini')\n",
    "\n",
    "# Training phase\n",
    "CART_classifier = CART_classifier.fit(predictor_train,target_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "model_prediction = CART_classifier.predict(predictor_test)\n",
    "\n",
    "# Print the training accuracy \n",
    "print(\"Model accuracy:\", metrics.accuracy_score(target_test, model_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e416cae",
   "metadata": {},
   "source": [
    "## Decision tree weaknesses \n",
    "Where CART excells in its interpretability and ease of implementation, it often performs poorly when trying to **generalize to new data.** Given how CART discriminates purely based on the data features, a single tree if allowed to grow uninterrupted to any depth will often assume high variance (i.e overfit the data.) This issue is in fact not specific to CART, but decision tree models in general. \n",
    "\n",
    "Fortunately there are a number of methods at our disposal to combat these issues. These usually involve reducing the tree structure complexity using **pruning methods.** Despite pruning, a single decision tree typically will not perform very well at classification and regression tasks. Fortunately, producing multiple decision trees and taking a consensus from them has proven a far more succesful modelling strategy. These multiple tree strategies are broadly classified as **ensemble methods.** We touch upon different ensemble methods in a separate notebook.\n",
    "\n",
    "## Improving accuracy by reducing variance\n",
    "With our current model, we are getting close to a 70% accuracy. We suspect there is some overfitting happening which is leading to poor generalization to the test set. As such, we can employ a pruning technique to reduce the complexity of the decision boundaries drawn by our model. One technique is **limiting tree depth.** By restricting tree depth, we are implicitly reducing the number of decision boundaries drawn by the tree, and hopefully improving the models ability to generalize to new data. \n",
    "\n",
    "We can pass tree depth directly as an argument to the DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9aeb6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.7987012987012987\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "CART_classifier = DecisionTreeClassifier(criterion = 'gini', max_depth = 2)\n",
    "\n",
    "# Redo training and predictions\n",
    "CART_classifier = CART_classifier.fit(predictor_train,target_train)\n",
    "model_prediction = CART_classifier.predict(predictor_test)\n",
    "\n",
    "# Print the training accuracy \n",
    "print(\"Model accuracy:\", metrics.accuracy_score(target_test, model_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2464ec",
   "metadata": {},
   "source": [
    "Tree depth as such becomes a hyperparamter that can be tuned. Note that tree depth is one of a few pruning techniques offered by scikit-learn.  *Cost complexity (ccp_alfa)* is another common post pruning technique where a fully grown tree is reduced to subtrees by removing nodes to reduce the error rate. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
