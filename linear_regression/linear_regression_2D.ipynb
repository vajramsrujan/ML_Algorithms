{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c78510",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "## Introduction\n",
    "I'm using this notebook as a means to serve my own understanding for linear regressors. Hopefully, it is a useful find for anyone who is looking to understand these models as well. \n",
    "\n",
    "## Key idea\n",
    "Linear regressors are a great way to model a linear relationship between two or more variables. In other words, we are attempting to derive a function for a straight line which most accurately captures the relationship between two or more sets of data. Linear regressors offer a mathematically rigorous approach to finding this 'best fit' line. \n",
    "\n",
    "## Mathematics of linear regression in 2D space\n",
    "We know that a 2D line can be described using the equation\n",
    "\n",
    "$y = mx + b$\n",
    "\n",
    "* y = y coordinate \n",
    "* m = slope \n",
    "* x = x coordinate\n",
    "* b = y intercept\n",
    "\n",
    "The goal is to arrive at a value of m and b that best fits our data, (in this case, we are assuming two variables). In other words, we must **minimize the distances between the data points and the predicted line.**\n",
    "We can formulate this as a simple optimization problem: \n",
    "\n",
    "For what values of $m$ and $b$ does $\\sum \\limits _{i=1} ^{n} (y_{i} - (mx_{i} + b)) ^2$ become the smallest possible value? Here, $y_{i}$ is the i'th data points y value, and $mx_{i} + b$ represents the predicted y value. The sum term is simply adding up all of the squared y distances between each predicted point, and actual data point. \n",
    "\n",
    "The above equation is called the *square error*, and typically in this kind of an optimization problem, we also throw in a $1/n$ term, which just changes the expression to a *mean square error* i.e\\\n",
    "$Error_{mse} = (1/n)\\sum \\limits _{i=1}^{n} (y_{i} - (mx_{i} + b)) ^2$\n",
    "\n",
    "We can use **partial derivaties** to compute the slop of this error function with respect to $m$ and $b$. This is useful because it gives us an idea of what *direction* to nudge the variables in to get a smaller error. \n",
    "\n",
    "$\\frac{\\partial E}{\\partial m} = (1/n)\\sum \\limits _{i=1}^{n} (2)(y_{i} - (mx_{i} + b))(-x_{i})$\\\n",
    "$\\frac{\\partial E}{\\partial b} = (1/n)\\sum \\limits _{i=1}^{n} (2)(y_{i} - (mx_{i} + b))(-1)$\n",
    "\n",
    "In essence, each partial derivative describes the slope of the error function. We can use this to update our variables to values that will reduce this error function:\n",
    "\n",
    "$m = m -(L)\\frac{\\partial E}{\\partial m}$\\\n",
    "$b = b -(L)\\frac{\\partial b}{\\partial m}$\n",
    "\n",
    "Here, $L$ is a constant called the **Learning rate**. It can be tuned to modify how big of an update is made to each of the variables in our hunt for the error functions minimum value. Notice we are *subtracting* the partial derivative, because the partial derivative by itself gives us the steepest *ascent*, (change that most greatly *increases* the error function.) We want to go in the opposite direction.\n",
    "\n",
    "The idea here is to **iteratively update our m and b values** until we reach some kind of minima. We could check for convergence by inspecting how much the mean squared error changes per update. If we find that the error is barely moving after many updates, we can conclude that we have reached a minima. In 2D linear regressor models, this is usually the global minima. \n",
    "\n",
    "## Dataset for demonstration\n",
    "I decided to use a fish market dataset which catalogues the weight, lengths and prices of various fish species: \n",
    "\n",
    "https://www.kaggle.com/aungpyaeap/fish-market\n",
    "\n",
    "I think as a starting exercise it would be interesting to try and model the relationship between the weight and vertical length of the Perch fish species. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e0af867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fish dataset\n",
    "fish_data = pd.read_csv(\"/Users/srujanvajram/Documents/Github/ML_playground/ML_playground/linear_regression/Fish.csv\")\n",
    "\n",
    "# Index the pandas dataframes for Perch weight and height\n",
    "mask = fish_data['Species'] == 'Perch'\n",
    "perch_data = fish_data[mask]\n",
    "perch_weight = perch_data['Weight'].values\n",
    "perch_height = perch_data['Height'].values \n",
    "\n",
    "# We first scale the data by dividing with the largest observed value\n",
    "# This has significant implications for our model down the line\n",
    "perch_weight = perch_weight/max(perch_weight)\n",
    "perch_height = perch_height/max(perch_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2b88690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Scaled Height')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the scaled weights and heights as a scatter plot\n",
    "plt.scatter(perch_weight,perch_height, s=5)\n",
    "plt.xlabel(\"Scaled Weight\")\n",
    "plt.ylabel(\"Scaled Height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cea299",
   "metadata": {},
   "source": [
    "## First impressions\n",
    "This is pretty interesting distribution. From initial inspection I can already tell this not quite a linear relationship (a logarithmic curve would fit this much better) but for the purpose of this exercise, lets see how good of a linear fit we can model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23c1c4",
   "metadata": {},
   "source": [
    "## Updating the variables (Gradient Descent)\n",
    "The process of updating the variables in the direction of steepest descent is called **gradient descent.** We compose a function to update our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "278ffdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,b,x,y,L):\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions = m*x + b\n",
    "    # Compute the gradient of the loss for all predicted points \n",
    "    m_grad = (-2/len(x)) * sum(  (y -(predictions)) * x )\n",
    "    b_grad = (-2/len(x)) * sum(  (y -(predictions)) )\n",
    "        \n",
    "    m_new = m - L*(m_grad)\n",
    "    b_new = b - L*(b_grad)\n",
    "    \n",
    "    return m_new, b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc0177",
   "metadata": {},
   "source": [
    "## Running gradient descent \n",
    "Now that we have composed the gradient descent function, we simply need to update our variables many times. This lets us reach an iteratively smaller and smaller loss. For this exercise, were running our descent function an arbitrary number of times. I've chosen 1000, though you could very much have chosen 100, or 10,000. The more number of iterations, the more confident you can be that we have reached a minima in the loss function, the caveat being it takes longer to run. Each interation and subsequent update is called an **epoch**. We run gradient descent for 1000 epochs. \n",
    "\n",
    "## The importance of scaling\n",
    "It is important we scaled our data prior to gradient descent. Vastly different scales across variables can cause instability when computing gradient descent, and lead to an 'exploding' or 'vanishing' gradient where the computed slopes are too large or too small. For instance, if fish weight was recorded in milligrams (values ranging in the tens of thousands), whereas height was in cm (vales ranging in the hundreds) our computed gradients would be incredibly large or small due to the way the data is distributed. As such, it is essential we scale the data down so that the feature spaces are comprable in range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86972d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradient_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_2684/2830312389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Notice how we update m and b, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# then use the same values of m and b in the next gradient descent call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperch_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperch_height\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# # Finally, plot the original data, and the line predicted using our linear regression model:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gradient_descent' is not defined"
     ]
    }
   ],
   "source": [
    "# Initializing starting values \n",
    "m = 0\n",
    "b = 0\n",
    "L = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Run for 1000 epochs\n",
    "for i in range(epochs): \n",
    "    # Notice how we update m and b, but \n",
    "    # then use the same values of m and b in the next gradient descent call\n",
    "    m, b = gradient_descent(m,b,perch_weight,perch_height,L)\n",
    "    \n",
    "# # Finally, plot the original data, and the line predicted using our linear regression model: \n",
    "linear_model = m*perch_weight + b\n",
    "# Plot the scaled weights and heights as a scatter plot\n",
    "plt.scatter(perch_weight,perch_height, s=5)\n",
    "plt.xlabel(\"Scaled Weight\")\n",
    "plt.ylabel(\"Scaled Height\")\n",
    "plt.plot(perch_weight, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fcdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
