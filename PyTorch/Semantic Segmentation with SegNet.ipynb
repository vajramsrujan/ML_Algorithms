{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993b69f0",
   "metadata": {},
   "source": [
    "# Semantic Segmentation using SegNet \n",
    "## Introduction\n",
    "This notebook is a summary of a personal project demonstrating how convolutional neural networks (CNNs) can be used for an image segmentation task. Specifically, we describe semantic labelling of specific targets within a dataset. Semantic segmentation is a common segmentation technique where different objects are distinguised from each other based on class type only, though each instance within the class is not delineated. \n",
    "\n",
    "<br>\n",
    "<img src=\"typesofseg.png\" width=1000 height=1000 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e5682",
   "metadata": {},
   "source": [
    "##  Dataset \n",
    "I've been particularly interested in how supervised learning can be used in digital pathology and other medical/research image modailties. For this project, I used a **darkfield microscopy dataset** populated with labelled red blood cells (RBC's) and Spirochaeta bacteria. My goal was to construct and test a CNN architecture from the groud up which could label RBC's from bacteria within reasonable accuracy. For me, automating labelling of bacterium species seemed like a great starting point for exploring the capabilities of supervised learning in a diagnostic capacity. \n",
    "\n",
    "I later discuss the results on my implementation in PyTorch. \n",
    "\n",
    "### Images and mask labels\n",
    "\n",
    "A few examples from the dataset of of colorized cells and bacterium species (left) and corresponding labelled masks which would be used for training (right) \n",
    "<br>\n",
    "<img src=\"1.png\" width=400 height=400 />\n",
    "<img src=\"2.png\" width=400 height=400 />\n",
    "<img src=\"3.png\" width=400 height=400 />\n",
    "\n",
    "### Overview\n",
    "<img src=\"overview.png\" width=700 height=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97bc2f",
   "metadata": {},
   "source": [
    "## SegNet \n",
    "SegNet is an CNN configuration originally developed at the University of Cambridge for semantic segmentation. At the time of its publication, SegNet introduced a rather novel *encoding-decoding architecture* compared to its contemporaries, which used fully connected layers (and consequently many more parameters to train.) As for why I chose this architecture, it came down to my personal interest with its implementation. \n",
    "\n",
    "### Architecture\n",
    "The encoding - decoding stragety aims to reduce the number of learnable parameters by sucessively reducing spatial resolution in the encoder stage, followed by restoring this resolution for the final label projection. We trade spatial resolution for feature map depth at each encoder stage to encourage the network to infer more complex abstractions (ie. patterns in the image data.) The decoder network inverts this process by restoring resolution and collapsing feature map depth to project a final prediction. \n",
    "\n",
    "The complete architecture is illustrated below from the original SegNet paper: \n",
    "<img src=\"segnet_architecture.png\" width=800 height=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3dbd8",
   "metadata": {},
   "source": [
    "- **Convolutional layer**: \n",
    "A layer where the input image is convolved with a pre-determined filter of size K, stride S, and padding P. Convolutions are essentially linear combination operations with the filter providing the weights. \n",
    "<img src=\"05_convolutions_example.gif\" width=\"500\" align=\"center\"> _Stride_ refers to how many units the kernel jumps either vertically or horizontally along the input image during each convolution operation. _Padding_ refers to how many rows and columns of zeros are added around the input matrix prior to convolution. Since colvolutions inherently reduce the input size, we can pad in case we would like the same output dimension. The filter weights are leanable parameters, meaning they can be modified as part of the networks learning process (back propagation.) \n",
    "\n",
    "\n",
    "- **Batch normalization layer**: \n",
    "A layer which normalizes the input images provided to it. This layer typically accepts a 'mini-batch' of images prior to normalizing the images using their mean and standard deviation. This normalization is important so that the feature spaces are comprable in range, as otherwise we run the risk of encoutering *vanishing and exploding gradients* in the backpropagation step (i.e gradients are too small/large and training slows down or effectively stops.) \n",
    "\n",
    "\n",
    "- **ReLU**: \n",
    "Short form for rectified linear unit function. It is an element-wise activation function which applies a very simple scaling to its input: <img src=\"relu.png\" width=300 height=300 />\n",
    "Each element in an input layer fed into a ReLU is subject to this activation function. This function is only piecewise linear (note the elbow at the origin.) ReLU adds a non-linearity to the network and allows the netowrk to learn non-linear relationships in the data.\n",
    "\n",
    "\n",
    "- **Max Pooling and Max Unpooling**: \n",
    "_Max Pooling_ is a downsampling operation where the maximum element is taken in a sliding kernel of size K, with stride S, and padding P (of the input image.) Max pooling is important in the SegNet architecture as part of the encoder stage to reduce spatial resolution. _Max Unpooling_ is the inverse of this operation, which is significant in the decoder stage where we attempt to restore spatial resolution prior to our pixel-wise class predictions. <img src=\"pooling.png\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "- **Softmax**:  \n",
    "Function which turns an input vector of 'n' values into values that add up to '1'. The softmax is best demonstrated using a simple example: Assume some vector K = [a, b, c] \n",
    "### $Softmax(k) =$ [$\\frac{e^a}{e^a + e^b + e^c}$, $\\frac{e^b}{e^a + e^b + e^c}$, $\\frac{e^c}{e^a + e^b + e^c}$] \n",
    "\n",
    "    The softmax function has the effect of scaling values into a probability distribution, where negative values return small pobabilities, and vice versa. As such, we can use it to produce classification decisions for any number of mutually exclusive classes. Since our output from SegNet will contain one channel per class,  we use it to produce a probability distribution for each channel, with each channel representing the probability that a pixel belongs to a its class. Once we obtain this, we simply label any pixel based on which channel assigned the highest probability to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e27db",
   "metadata": {},
   "source": [
    "## Training and validation using PyTorch\n",
    "The complete code can be found at under the SegNet folder. Here, I describe the complete workflow of my PyTorch implemetation, including the transformations used on the training data, the performance of the model, and the decisions I made in tuning the model. \n",
    "\n",
    "#### Pytorch overview\n",
    "Our PyTorch SegNet project consists of a several key components: \n",
    "1. Dataset\n",
    "2. DataLoaders\n",
    "3. Data transformations\n",
    "4. Model architecture\n",
    "5. Loss function and optimizer\n",
    "6. Hyperparameters\n",
    "7. Validating accuracy\n",
    "\n",
    "#### 1. Dataset\n",
    "I write a custom dataset named *SegNetDataSet* to load my images and target masks, inheriting from the *Dataset* class. The code can be found here. My dataset class accepts transform arguments for both the images and target masks separately, since there are some transformations I do not apply to the target masks (ex: no batch normalization to the target mask.)  \n",
    "\n",
    "```python\n",
    "# Load custom dataset\n",
    "dataset = SegNetDataSet(r'C:\\Users\\vajra\\Documents\\GitHub\\ML_playground\\PyTorch\\segnet\\archive', \n",
    "                        data_transforms=data_transforms, target_transforms=target_transforms)\n",
    "```\n",
    "\n",
    "#### 2. DataLoaders\n",
    "I use the standard DataLoader class to create two loaders, namely a training and test loader for the training and test sets respectively. \n",
    "\n",
    "```python\n",
    "# Produce test and train sets\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [329, 37]) # 90% 10% split between train and test \n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "```\n",
    "\n",
    "#### 3. Data Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e9aaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "    \n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86067d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b31ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
